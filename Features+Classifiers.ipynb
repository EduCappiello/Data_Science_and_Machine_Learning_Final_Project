{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.signal import welch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import read_data as rd  # Importing the read_data.py module\n",
    "from scipy.fft import fft\n",
    "from scipy.stats import skew, kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize the data using min-max scaling\n",
    "def normalize_data(data):\n",
    "    min_val = np.min(data, axis=0)\n",
    "    max_val = np.max(data, axis=0)\n",
    "    normalized = (data - min_val) / (max_val - min_val)\n",
    "    return normalized\n",
    "\n",
    "def extract_time_domain_features(signal):\n",
    "    features = []\n",
    "    for i in range(signal.shape[1]):\n",
    "        sig = signal[:, i]\n",
    "        features.extend([\n",
    "            np.mean(sig),               # Mean\n",
    "            np.std(sig),                # Standard Deviation\n",
    "            skew(sig),                  # Skewness\n",
    "            kurtosis(sig),              # Kurtosis\n",
    "            np.max(sig),                # Maximum\n",
    "            np.min(sig),                # Minimum\n",
    "            np.ptp(sig),                # Peak-to-Peak\n",
    "            np.sqrt(np.mean(sig**2)),   # RMS\n",
    "            np.sum(np.abs(np.diff(np.sign(sig)))) / 2,  # Zero Crossing Rate\n",
    "            np.sum(sig**2)              # Energy\n",
    "        ])\n",
    "    return features\n",
    "\n",
    "def extract_fft_features(signal, fs, top_n=50):\n",
    "    N = len(signal)\n",
    "    T = 1.0 / fs\n",
    "    yf = fft(signal)\n",
    "    xf = np.fft.fftfreq(N, T)[:N//2]\n",
    "    yf = 2.0/N * np.abs(yf[:N//2])\n",
    "    \n",
    "    # Get indices of the top_n highest frequencies\n",
    "    top_indices = np.argsort(yf)[-top_n:]\n",
    "    \n",
    "    # Extract the top_n highest FFT features\n",
    "    fft_features = yf[top_indices]\n",
    "    return fft_features\n",
    "\n",
    "def extract_frequency_domain_features(signal, fs):\n",
    "    features = []\n",
    "    for i in range(signal.shape[1]):\n",
    "        sig = signal[:, i]\n",
    "        freqs, psd = welch(sig, fs)\n",
    "        \n",
    "        features.extend([\n",
    "            np.mean(psd),                   # Mean Power Spectral Density\n",
    "            np.sum(psd),                    # Total Power\n",
    "            np.argmax(psd),                 # Peak Frequency\n",
    "            np.mean(freqs * psd) / np.mean(psd),  # Spectral Centroid\n",
    "            np.sqrt(np.mean((freqs - np.mean(freqs))**2 * psd)) / np.mean(psd),  # Spectral Bandwidth\n",
    "            np.percentile(psd, 75) - np.percentile(psd, 25),  # Spectral Contrast\n",
    "            np.max(freqs[np.cumsum(psd) / np.sum(psd) <= 0.85])  # Spectral Roll-off\n",
    "        ])\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dataset_dir = '/home/ecappiell/datasets/full'\n",
    "data_arrays, labels, class_ids = rd.process_mafaulda_data(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original sampling rate (in Hz)\n",
    "original_sampling_rate = 50 * 10**3  # 50 kHz\n",
    "\n",
    "# Target sampling rate (in Hz)\n",
    "target_sampling_rate = 2 * 10**3  # 2 kHz\n",
    "\n",
    "# Downsample the data\n",
    "downsampled_data = rd.downsample_data(data_arrays, original_sampling_rate, target_sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the downsampled d ata\n",
    "normalized_data = np.array([normalize_data(signal) for signal in downsampled_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for each signal\n",
    "X = []\n",
    "for signal in normalized_data:\n",
    "    time_features = extract_time_domain_features(signal)\n",
    "    fft_features = [extract_fft_features(signal[:, i], target_sampling_rate) for i in range(signal.shape[1])]\n",
    "    freq_features = extract_frequency_domain_features(signal, target_sampling_rate)\n",
    "    signal_features = np.concatenate((time_features, np.hstack(fft_features), freq_features))\n",
    "    X.append(signal_features)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming labels and class_ids are prepared for classification\n",
    "y = np.array(class_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the feature matrix using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Nearest Neighbors\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.92      0.79        59\n",
      "           1       0.92      0.95      0.94       100\n",
      "           2       0.90      0.60      0.72        15\n",
      "           3       0.95      0.94      0.94       154\n",
      "           4       0.99      0.91      0.95       168\n",
      "           5       0.92      0.91      0.92        90\n",
      "\n",
      "    accuracy                           0.92       586\n",
      "   macro avg       0.90      0.87      0.88       586\n",
      "weighted avg       0.93      0.92      0.92       586\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 54   1   0   1   1   2]\n",
      " [  2  95   0   1   0   2]\n",
      " [  2   1   9   0   0   3]\n",
      " [  9   0   0 144   1   0]\n",
      " [  5   6   0   4 153   0]\n",
      " [  6   0   1   1   0  82]]\n",
      "==================================================\n",
      "Classifier: Linear SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98        59\n",
      "           1       0.96      0.98      0.97       100\n",
      "           2       0.86      0.80      0.83        15\n",
      "           3       0.97      0.97      0.97       154\n",
      "           4       0.98      0.95      0.97       168\n",
      "           5       0.99      0.99      0.99        90\n",
      "\n",
      "    accuracy                           0.97       586\n",
      "   macro avg       0.95      0.95      0.95       586\n",
      "weighted avg       0.97      0.97      0.97       586\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 59   0   0   0   0   0]\n",
      " [  0  98   1   0   0   1]\n",
      " [  2   1  12   0   0   0]\n",
      " [  1   0   0 150   3   0]\n",
      " [  0   2   1   5 160   0]\n",
      " [  0   1   0   0   0  89]]\n",
      "==================================================\n",
      "Classifier: RBF SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        59\n",
      "           1       0.00      0.00      0.00       100\n",
      "           2       0.00      0.00      0.00        15\n",
      "           3       0.00      0.00      0.00       154\n",
      "           4       0.29      1.00      0.45       168\n",
      "           5       0.00      0.00      0.00        90\n",
      "\n",
      "    accuracy                           0.29       586\n",
      "   macro avg       0.05      0.17      0.07       586\n",
      "weighted avg       0.08      0.29      0.13       586\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  0   0   0   0  59   0]\n",
      " [  0   0   0   0 100   0]\n",
      " [  0   0   0   0  15   0]\n",
      " [  0   0   0   0 154   0]\n",
      " [  0   0   0   0 168   0]\n",
      " [  0   0   0   0  90   0]]\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecappiell/dev/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ecappiell/dev/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ecappiell/dev/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Gaussian Process\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.02      0.03        59\n",
      "           1       0.00      0.00      0.00       100\n",
      "           2       0.00      0.00      0.00        15\n",
      "           3       1.00      0.01      0.01       154\n",
      "           4       1.00      0.01      0.02       168\n",
      "           5       0.15      1.00      0.27        90\n",
      "\n",
      "    accuracy                           0.16       586\n",
      "   macro avg       0.53      0.17      0.06       586\n",
      "weighted avg       0.67      0.16      0.05       586\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  1   0   0   0   0  58]\n",
      " [  0   0   0   0   0 100]\n",
      " [  0   0   0   0   0  15]\n",
      " [  0   0   0   1   0 153]\n",
      " [  0   0   0   0   2 166]\n",
      " [  0   0   0   0   0  90]]\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecappiell/dev/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ecappiell/dev/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ecappiell/dev/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.86      0.75        59\n",
      "           1       0.94      0.84      0.89       100\n",
      "           2       0.50      0.13      0.21        15\n",
      "           3       0.85      0.78      0.81       154\n",
      "           4       0.75      0.86      0.80       168\n",
      "           5       0.84      0.76      0.80        90\n",
      "\n",
      "    accuracy                           0.80       586\n",
      "   macro avg       0.76      0.71      0.71       586\n",
      "weighted avg       0.81      0.80      0.80       586\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 51   0   0   4   1   3]\n",
      " [  0  84   0   0  12   4]\n",
      " [  1   0   2   0   9   3]\n",
      " [  6   4   1 120  22   1]\n",
      " [ 11   1   0   9 145   2]\n",
      " [  8   0   1   9   4  68]]\n",
      "==================================================\n",
      "Classifier: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.10      0.18        59\n",
      "           1       0.69      0.67      0.68       100\n",
      "           2       0.00      0.00      0.00        15\n",
      "           3       0.64      0.79      0.71       154\n",
      "           4       0.65      0.86      0.74       168\n",
      "           5       0.72      0.52      0.61        90\n",
      "\n",
      "    accuracy                           0.66       586\n",
      "   macro avg       0.56      0.49      0.49       586\n",
      "weighted avg       0.65      0.66      0.62       586\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  6   9   0  21  15   8]\n",
      " [  0  67   0   9  21   3]\n",
      " [  1   7   0   4   3   0]\n",
      " [  1   0   0 122  24   7]\n",
      " [  0  11   0  12 145   0]\n",
      " [  1   3   0  24  15  47]]\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecappiell/dev/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ecappiell/dev/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ecappiell/dev/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ecappiell/dev/.venv/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: AdaBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.68      0.65        59\n",
      "           1       0.50      0.89      0.64       100\n",
      "           2       0.38      0.20      0.26        15\n",
      "           3       0.71      0.55      0.62       154\n",
      "           4       0.66      0.51      0.58       168\n",
      "           5       0.76      0.73      0.75        90\n",
      "\n",
      "    accuracy                           0.63       586\n",
      "   macro avg       0.60      0.59      0.58       586\n",
      "weighted avg       0.65      0.63      0.62       586\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  0  0  1  5 13]\n",
      " [ 0 89  2  6  3  0]\n",
      " [ 0  1  3  2  8  1]\n",
      " [ 1 41  1 84 21  6]\n",
      " [ 9 46  2 24 86  1]\n",
      " [14  0  0  2  8 66]]\n",
      "==================================================\n",
      "Classifier: Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.69      0.46        59\n",
      "           1       0.83      0.86      0.85       100\n",
      "           2       0.35      0.73      0.48        15\n",
      "           3       0.82      0.63      0.71       154\n",
      "           4       0.87      0.58      0.69       168\n",
      "           5       0.76      0.87      0.81        90\n",
      "\n",
      "    accuracy                           0.70       586\n",
      "   macro avg       0.66      0.73      0.67       586\n",
      "weighted avg       0.77      0.70      0.71       586\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  0  5  0  2 11]\n",
      " [ 1 86  3  3  5  2]\n",
      " [ 1  1 11  0  0  2]\n",
      " [39  2  5 97  3  8]\n",
      " [31 14  6 18 97  2]\n",
      " [ 5  0  1  1  5 78]]\n",
      "==================================================\n",
      "Classifier: QDA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.10      0.13        59\n",
      "           1       0.32      0.23      0.27       100\n",
      "           2       0.05      0.07      0.06        15\n",
      "           3       0.32      0.39      0.35       154\n",
      "           4       0.47      0.61      0.53       168\n",
      "           5       0.28      0.18      0.22        90\n",
      "\n",
      "    accuracy                           0.36       586\n",
      "   macro avg       0.27      0.26      0.26       586\n",
      "weighted avg       0.34      0.36      0.34       586\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  6   7   3  24   9  10]\n",
      " [  6  23   4  30  30   7]\n",
      " [  1   1   1   7   4   1]\n",
      " [  8  16   1  60  56  13]\n",
      " [  3  12   5  34 103  11]\n",
      " [  8  13   5  31  17  16]]\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecappiell/dev/.venv/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:949: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025,probability=True),\n",
    "    SVC(gamma=2, C=1, probability=True),\n",
    "    GaussianProcessClassifier(),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "classifier_names = [\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "    \"Gaussian Process\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\"]\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for name, clf in zip(classifier_names, classifiers):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"Classifier: {name}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
