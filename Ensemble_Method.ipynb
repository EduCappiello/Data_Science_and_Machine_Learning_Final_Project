{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.signal import welch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import read_data as rd  # Importing the read_data.py module\n",
    "from scipy.fft import fft\n",
    "from scipy.stats import skew, kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize the data using min-max scaling\n",
    "def normalize_data(data):\n",
    "    min_val = np.min(data, axis=0)\n",
    "    max_val = np.max(data, axis=0)\n",
    "    normalized = (data - min_val) / (max_val - min_val)\n",
    "    return normalized\n",
    "\n",
    "def extract_time_domain_features(signal):\n",
    "    features = []\n",
    "    for i in range(signal.shape[1]):\n",
    "        sig = signal[:, i]\n",
    "        features.extend([\n",
    "            np.mean(sig),               # Mean\n",
    "            np.std(sig),                # Standard Deviation\n",
    "            skew(sig),                  # Skewness\n",
    "            kurtosis(sig),              # Kurtosis\n",
    "            np.max(sig),                # Maximum\n",
    "            np.min(sig),                # Minimum\n",
    "            np.ptp(sig),                # Peak-to-Peak\n",
    "            np.sqrt(np.mean(sig**2)),   # RMS\n",
    "            np.sum(np.abs(np.diff(np.sign(sig)))) / 2,  # Zero Crossing Rate\n",
    "            np.sum(sig**2)              # Energy\n",
    "        ])\n",
    "    return features\n",
    "\n",
    "def extract_fft_features(signal, fs, top_n=50):\n",
    "    N = len(signal)\n",
    "    T = 1.0 / fs\n",
    "    yf = fft(signal)\n",
    "    xf = np.fft.fftfreq(N, T)[:N//2]\n",
    "    yf = 2.0/N * np.abs(yf[:N//2])\n",
    "    \n",
    "    # Get indices of the top_n highest frequencies\n",
    "    top_indices = np.argsort(yf)[-top_n:]\n",
    "    \n",
    "    # Extract the top_n highest FFT features\n",
    "    fft_features = yf[top_indices]\n",
    "    return fft_features\n",
    "\n",
    "def extract_frequency_domain_features(signal, fs):\n",
    "    features = []\n",
    "    for i in range(signal.shape[1]):\n",
    "        sig = signal[:, i]\n",
    "        freqs, psd = welch(sig, fs)\n",
    "        \n",
    "        features.extend([\n",
    "            np.mean(psd),                   # Mean Power Spectral Density\n",
    "            np.sum(psd),                    # Total Power\n",
    "            np.argmax(psd),                 # Peak Frequency\n",
    "            np.mean(freqs * psd) / np.mean(psd),  # Spectral Centroid\n",
    "            np.sqrt(np.mean((freqs - np.mean(freqs))**2 * psd)) / np.mean(psd),  # Spectral Bandwidth\n",
    "            np.percentile(psd, 75) - np.percentile(psd, 25),  # Spectral Contrast\n",
    "            np.max(freqs[np.cumsum(psd) / np.sum(psd) <= 0.85])  # Spectral Roll-off\n",
    "        ])\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dataset_dir = '/home/ecappiell/datasets/full'\n",
    "data_arrays, labels, class_ids = rd.process_mafaulda_data(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original sampling rate (in Hz)\n",
    "original_sampling_rate = 50 * 10**3  # 50 kHz\n",
    "\n",
    "# Target sampling rate (in Hz)\n",
    "target_sampling_rate = 2 * 10**3  # 2 kHz\n",
    "\n",
    "# Downsample the data\n",
    "downsampled_data = rd.downsample_data(data_arrays, original_sampling_rate, target_sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the downsampled d ata\n",
    "normalized_data = np.array([normalize_data(signal) for signal in downsampled_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for each signal\n",
    "X = []\n",
    "for signal in normalized_data:\n",
    "    time_features = extract_time_domain_features(signal)\n",
    "    fft_features = [extract_fft_features(signal[:, i], target_sampling_rate) for i in range(signal.shape[1])]\n",
    "    freq_features = extract_frequency_domain_features(signal, target_sampling_rate)\n",
    "    signal_features = np.concatenate((time_features, np.hstack(fft_features), freq_features))\n",
    "    X.append(signal_features)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming labels and class_ids are prepared for classification\n",
    "y = np.array(class_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the feature matrix using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025,probability=True),\n",
    "    SVC(gamma=2, C=1, probability=True),\n",
    "    GaussianProcessClassifier(),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "classifier_names = [\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "    \"Gaussian Process\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\"]\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for name, clf in zip(classifier_names, classifiers):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"Classifier: {name}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Train each classifier separately\n",
    "predictions = []\n",
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict_proba(X_test)[:, 1]  # Use probabilities as predictions\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "# Convert predictions to numpy array\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train_one_hot = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_one_hot = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Create a neural network for ensemble learning\n",
    "ensemble_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(len(classifiers),)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')  # Use softmax for multiclass classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "ensemble_model.compile(optimizer='adam',\n",
    "                       loss='categorical_crossentropy',\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the combined predictions\n",
    "ensemble_model.fit(predictions.T, y_train_one_hot, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "ensemble_predictions = ensemble_model.predict(predictions.T)\n",
    "ensemble_predictions = np.argmax(ensemble_predictions, axis=1)\n",
    "print(\"Ensemble Model Performance:\")\n",
    "print(classification_report(y_test, ensemble_predictions))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, ensemble_predictions))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
